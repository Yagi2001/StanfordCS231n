{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fdca1c",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f12280f",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab106c82",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721c3cb",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42765cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.289506\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88764e",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in* \n",
    "Our normalized scores at the start are 0.1 for almost all of them.Our loss function is -log(normalized true score) which is - log(0.1). The reason that all of scores are very close to 0.1 is : We choose our weights accordingly. Our weights at the start are very low values and they also have mean 0 and standart deviation 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2626ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.548480 analytic: 0.548480, relative error: 4.595697e-08\n",
      "numerical: -0.222239 analytic: -0.222239, relative error: 9.317044e-08\n",
      "numerical: 0.991745 analytic: 0.991745, relative error: 1.176224e-08\n",
      "numerical: -0.769438 analytic: -0.769438, relative error: 4.314047e-08\n",
      "numerical: 0.926140 analytic: 0.926140, relative error: 1.317819e-08\n",
      "numerical: -0.138194 analytic: -0.138194, relative error: 9.542646e-08\n",
      "numerical: 3.766723 analytic: 3.766723, relative error: 1.047367e-08\n",
      "numerical: -0.421055 analytic: -0.421055, relative error: 4.086365e-10\n",
      "numerical: 0.902850 analytic: 0.902850, relative error: 2.906362e-08\n",
      "numerical: -1.136210 analytic: -1.136210, relative error: 4.545739e-09\n",
      "numerical: -0.322661 analytic: -0.322661, relative error: 2.569769e-08\n",
      "numerical: -1.298357 analytic: -1.298357, relative error: 4.463152e-08\n",
      "numerical: -0.510831 analytic: -0.510831, relative error: 2.224928e-08\n",
      "numerical: -0.931430 analytic: -0.931430, relative error: 3.655837e-08\n",
      "numerical: -0.703961 analytic: -0.703961, relative error: 4.283061e-08\n",
      "numerical: -1.201857 analytic: -1.201857, relative error: 2.463228e-08\n",
      "numerical: -0.065139 analytic: -0.065139, relative error: 1.608518e-07\n",
      "numerical: -1.139006 analytic: -1.139006, relative error: 3.114710e-08\n",
      "numerical: 0.453407 analytic: 0.453407, relative error: 8.085901e-08\n",
      "numerical: -0.493701 analytic: -0.493701, relative error: 5.367097e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434e757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.289506e+00 computed in 0.062997s\n",
      "vectorized loss: 2.289506e+00 computed in 0.013000s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5e374d",
   "metadata": {
    "tags": [
     "code"
    ],
    "test": "tuning"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 158.866348\n",
      "iteration 100 / 1500: loss 142.484707\n",
      "iteration 200 / 1500: loss 128.730474\n",
      "iteration 300 / 1500: loss 116.509081\n",
      "iteration 400 / 1500: loss 105.238130\n",
      "iteration 500 / 1500: loss 95.399762\n",
      "iteration 600 / 1500: loss 86.323400\n",
      "iteration 700 / 1500: loss 78.184270\n",
      "iteration 800 / 1500: loss 70.813979\n",
      "iteration 900 / 1500: loss 64.283145\n",
      "iteration 1000 / 1500: loss 58.330278\n",
      "iteration 1100 / 1500: loss 52.716912\n",
      "iteration 1200 / 1500: loss 47.847121\n",
      "iteration 1300 / 1500: loss 43.587595\n",
      "iteration 1400 / 1500: loss 39.593250\n",
      "iteration 0 / 1500: loss 67.292307\n",
      "iteration 100 / 1500: loss 63.531813\n",
      "iteration 200 / 1500: loss 60.535782\n",
      "iteration 300 / 1500: loss 58.388131\n",
      "iteration 400 / 1500: loss 56.179511\n",
      "iteration 500 / 1500: loss 53.704526\n",
      "iteration 600 / 1500: loss 51.652170\n",
      "iteration 700 / 1500: loss 49.440224\n",
      "iteration 800 / 1500: loss 47.668213\n",
      "iteration 900 / 1500: loss 45.493137\n",
      "iteration 1000 / 1500: loss 43.823830\n",
      "iteration 1100 / 1500: loss 42.288950\n",
      "iteration 1200 / 1500: loss 40.755941\n",
      "iteration 1300 / 1500: loss 39.012025\n",
      "iteration 1400 / 1500: loss 37.609337\n",
      "iteration 0 / 1500: loss 35.559822\n",
      "iteration 100 / 1500: loss 34.042257\n",
      "iteration 200 / 1500: loss 33.076814\n",
      "iteration 300 / 1500: loss 32.177661\n",
      "iteration 400 / 1500: loss 31.525145\n",
      "iteration 500 / 1500: loss 30.787443\n",
      "iteration 600 / 1500: loss 30.092484\n",
      "iteration 700 / 1500: loss 29.381638\n",
      "iteration 800 / 1500: loss 28.944289\n",
      "iteration 900 / 1500: loss 28.105484\n",
      "iteration 1000 / 1500: loss 27.828859\n",
      "iteration 1100 / 1500: loss 26.987188\n",
      "iteration 1200 / 1500: loss 26.280368\n",
      "iteration 1300 / 1500: loss 26.043370\n",
      "iteration 1400 / 1500: loss 25.540121\n",
      "iteration 0 / 1500: loss 312.227051\n",
      "iteration 100 / 1500: loss 254.253187\n",
      "iteration 200 / 1500: loss 207.651223\n",
      "iteration 300 / 1500: loss 169.846244\n",
      "iteration 400 / 1500: loss 138.989365\n",
      "iteration 500 / 1500: loss 114.066195\n",
      "iteration 600 / 1500: loss 93.466866\n",
      "iteration 700 / 1500: loss 76.834939\n",
      "iteration 800 / 1500: loss 63.220101\n",
      "iteration 900 / 1500: loss 51.934320\n",
      "iteration 1000 / 1500: loss 42.769444\n",
      "iteration 1100 / 1500: loss 35.410827\n",
      "iteration 1200 / 1500: loss 29.299486\n",
      "iteration 1300 / 1500: loss 24.326403\n",
      "iteration 1400 / 1500: loss 20.274556\n",
      "iteration 0 / 1500: loss 620.843098\n",
      "iteration 100 / 1500: loss 415.084714\n",
      "iteration 200 / 1500: loss 278.213232\n",
      "iteration 300 / 1500: loss 186.966309\n",
      "iteration 400 / 1500: loss 125.680633\n",
      "iteration 500 / 1500: loss 84.818758\n",
      "iteration 600 / 1500: loss 57.538195\n",
      "iteration 700 / 1500: loss 39.099635\n",
      "iteration 800 / 1500: loss 26.862472\n",
      "iteration 900 / 1500: loss 18.696780\n",
      "iteration 1000 / 1500: loss 13.231332\n",
      "iteration 1100 / 1500: loss 9.494885\n",
      "iteration 1200 / 1500: loss 7.061352\n",
      "iteration 1300 / 1500: loss 5.439094\n",
      "iteration 1400 / 1500: loss 4.316414\n",
      "iteration 0 / 1500: loss 1536.373929\n",
      "iteration 100 / 1500: loss 563.346788\n",
      "iteration 200 / 1500: loss 207.671369\n",
      "iteration 300 / 1500: loss 77.462717\n",
      "iteration 400 / 1500: loss 29.737081\n",
      "iteration 500 / 1500: loss 12.219484\n",
      "iteration 600 / 1500: loss 5.843553\n",
      "iteration 700 / 1500: loss 3.512805\n",
      "iteration 800 / 1500: loss 2.671967\n",
      "iteration 900 / 1500: loss 2.316208\n",
      "iteration 1000 / 1500: loss 2.258126\n",
      "iteration 1100 / 1500: loss 2.175618\n",
      "iteration 1200 / 1500: loss 2.140223\n",
      "iteration 1300 / 1500: loss 2.159394\n",
      "iteration 1400 / 1500: loss 2.147811\n",
      "iteration 0 / 1500: loss 3098.998889\n",
      "iteration 100 / 1500: loss 416.202407\n",
      "iteration 200 / 1500: loss 57.571508\n",
      "iteration 300 / 1500: loss 9.572068\n",
      "iteration 400 / 1500: loss 3.154406\n",
      "iteration 500 / 1500: loss 2.324346\n",
      "iteration 600 / 1500: loss 2.220332\n",
      "iteration 700 / 1500: loss 2.204699\n",
      "iteration 800 / 1500: loss 2.172879\n",
      "iteration 900 / 1500: loss 2.175456\n",
      "iteration 1000 / 1500: loss 2.185472\n",
      "iteration 1100 / 1500: loss 2.183459\n",
      "iteration 1200 / 1500: loss 2.216529\n",
      "iteration 1300 / 1500: loss 2.190807\n",
      "iteration 1400 / 1500: loss 2.218418\n",
      "iteration 0 / 1500: loss 6161.777312\n",
      "iteration 100 / 1500: loss 110.339434\n",
      "iteration 200 / 1500: loss 4.130720\n",
      "iteration 300 / 1500: loss 2.284847\n",
      "iteration 400 / 1500: loss 2.247760\n",
      "iteration 500 / 1500: loss 2.210003\n",
      "iteration 600 / 1500: loss 2.246548\n",
      "iteration 700 / 1500: loss 2.258709\n",
      "iteration 800 / 1500: loss 2.219277\n",
      "iteration 900 / 1500: loss 2.207906\n",
      "iteration 1000 / 1500: loss 2.236563\n",
      "iteration 1100 / 1500: loss 2.260241\n",
      "iteration 1200 / 1500: loss 2.243113\n",
      "iteration 1300 / 1500: loss 2.245445\n",
      "iteration 1400 / 1500: loss 2.251798\n",
      "iteration 0 / 1500: loss 159.943196\n",
      "iteration 100 / 1500: loss 129.632181\n",
      "iteration 200 / 1500: loss 106.198750\n",
      "iteration 300 / 1500: loss 86.890888\n",
      "iteration 400 / 1500: loss 71.200142\n",
      "iteration 500 / 1500: loss 58.505517\n",
      "iteration 600 / 1500: loss 48.344539\n",
      "iteration 700 / 1500: loss 39.679951\n",
      "iteration 800 / 1500: loss 32.683385\n",
      "iteration 900 / 1500: loss 27.102389\n",
      "iteration 1000 / 1500: loss 22.697622\n",
      "iteration 1100 / 1500: loss 18.691329\n",
      "iteration 1200 / 1500: loss 15.749668\n",
      "iteration 1300 / 1500: loss 13.256027\n",
      "iteration 1400 / 1500: loss 11.195269\n",
      "iteration 0 / 1500: loss 66.324691\n",
      "iteration 100 / 1500: loss 60.349047\n",
      "iteration 200 / 1500: loss 55.117994\n",
      "iteration 300 / 1500: loss 50.793861\n",
      "iteration 400 / 1500: loss 46.853375\n",
      "iteration 500 / 1500: loss 43.264793\n",
      "iteration 600 / 1500: loss 40.031124\n",
      "iteration 700 / 1500: loss 36.938384\n",
      "iteration 800 / 1500: loss 34.204368\n",
      "iteration 900 / 1500: loss 31.480833\n",
      "iteration 1000 / 1500: loss 29.384649\n",
      "iteration 1100 / 1500: loss 27.233142\n",
      "iteration 1200 / 1500: loss 25.198694\n",
      "iteration 1300 / 1500: loss 23.423489\n",
      "iteration 1400 / 1500: loss 21.678899\n",
      "iteration 0 / 1500: loss 36.262965\n",
      "iteration 100 / 1500: loss 32.879000\n",
      "iteration 200 / 1500: loss 31.201407\n",
      "iteration 300 / 1500: loss 29.919805\n",
      "iteration 400 / 1500: loss 28.451539\n",
      "iteration 500 / 1500: loss 27.500445\n",
      "iteration 600 / 1500: loss 26.310390\n",
      "iteration 700 / 1500: loss 25.395545\n",
      "iteration 800 / 1500: loss 24.435179\n",
      "iteration 900 / 1500: loss 23.355675\n",
      "iteration 1000 / 1500: loss 22.441763\n",
      "iteration 1100 / 1500: loss 21.614730\n",
      "iteration 1200 / 1500: loss 20.827979\n",
      "iteration 1300 / 1500: loss 19.866797\n",
      "iteration 1400 / 1500: loss 19.235141\n",
      "iteration 0 / 1500: loss 317.362463\n",
      "iteration 100 / 1500: loss 212.103170\n",
      "iteration 200 / 1500: loss 141.992834\n",
      "iteration 300 / 1500: loss 95.718757\n",
      "iteration 400 / 1500: loss 64.749853\n",
      "iteration 500 / 1500: loss 43.940215\n",
      "iteration 600 / 1500: loss 30.030169\n",
      "iteration 700 / 1500: loss 20.659097\n",
      "iteration 800 / 1500: loss 14.541112\n",
      "iteration 900 / 1500: loss 10.420925\n",
      "iteration 1000 / 1500: loss 7.554041\n",
      "iteration 1100 / 1500: loss 5.704509\n",
      "iteration 1200 / 1500: loss 4.559032\n",
      "iteration 1300 / 1500: loss 3.817691\n",
      "iteration 1400 / 1500: loss 3.120358\n",
      "iteration 0 / 1500: loss 619.852694\n",
      "iteration 100 / 1500: loss 277.311912\n",
      "iteration 200 / 1500: loss 125.307887\n",
      "iteration 300 / 1500: loss 57.128212\n",
      "iteration 400 / 1500: loss 26.794612\n",
      "iteration 500 / 1500: loss 13.113112\n",
      "iteration 600 / 1500: loss 7.061891\n",
      "iteration 700 / 1500: loss 4.233817\n",
      "iteration 800 / 1500: loss 3.095110\n",
      "iteration 900 / 1500: loss 2.505728\n",
      "iteration 1000 / 1500: loss 2.316684\n",
      "iteration 1100 / 1500: loss 2.136585\n",
      "iteration 1200 / 1500: loss 2.078594\n",
      "iteration 1300 / 1500: loss 2.103207\n",
      "iteration 1400 / 1500: loss 2.067216\n",
      "iteration 0 / 1500: loss 1517.133861\n",
      "iteration 100 / 1500: loss 204.269436\n",
      "iteration 200 / 1500: loss 29.116543\n",
      "iteration 300 / 1500: loss 5.719043\n",
      "iteration 400 / 1500: loss 2.603127\n",
      "iteration 500 / 1500: loss 2.190358\n",
      "iteration 600 / 1500: loss 2.160657\n",
      "iteration 700 / 1500: loss 2.172082\n",
      "iteration 800 / 1500: loss 2.106960\n",
      "iteration 900 / 1500: loss 2.164001\n",
      "iteration 1000 / 1500: loss 2.210409\n",
      "iteration 1100 / 1500: loss 2.157450\n",
      "iteration 1200 / 1500: loss 2.128725\n",
      "iteration 1300 / 1500: loss 2.184729\n",
      "iteration 1400 / 1500: loss 2.141233\n",
      "iteration 0 / 1500: loss 3064.625548\n",
      "iteration 100 / 1500: loss 55.853212\n",
      "iteration 200 / 1500: loss 3.153726\n",
      "iteration 300 / 1500: loss 2.230840\n",
      "iteration 400 / 1500: loss 2.187662\n",
      "iteration 500 / 1500: loss 2.224415\n",
      "iteration 600 / 1500: loss 2.216443\n",
      "iteration 700 / 1500: loss 2.197661\n",
      "iteration 800 / 1500: loss 2.201604\n",
      "iteration 900 / 1500: loss 2.229613\n",
      "iteration 1000 / 1500: loss 2.205808\n",
      "iteration 1100 / 1500: loss 2.177242\n",
      "iteration 1200 / 1500: loss 2.186415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.182424\n",
      "iteration 1400 / 1500: loss 2.240996\n",
      "iteration 0 / 1500: loss 6179.146783\n",
      "iteration 100 / 1500: loss 3.971535\n",
      "iteration 200 / 1500: loss 2.256333\n",
      "iteration 300 / 1500: loss 2.230292\n",
      "iteration 400 / 1500: loss 2.228915\n",
      "iteration 500 / 1500: loss 2.198387\n",
      "iteration 600 / 1500: loss 2.235338\n",
      "iteration 700 / 1500: loss 2.217730\n",
      "iteration 800 / 1500: loss 2.230304\n",
      "iteration 900 / 1500: loss 2.211346\n",
      "iteration 1000 / 1500: loss 2.239292\n",
      "iteration 1100 / 1500: loss 2.206558\n",
      "iteration 1200 / 1500: loss 2.236369\n",
      "iteration 1300 / 1500: loss 2.226131\n",
      "iteration 1400 / 1500: loss 2.216237\n",
      "iteration 0 / 1500: loss 157.721717\n",
      "iteration 100 / 1500: loss 104.730320\n",
      "iteration 200 / 1500: loss 70.345052\n",
      "iteration 300 / 1500: loss 47.682022\n",
      "iteration 400 / 1500: loss 32.440715\n",
      "iteration 500 / 1500: loss 22.303294\n",
      "iteration 600 / 1500: loss 15.441735\n",
      "iteration 700 / 1500: loss 11.036034\n",
      "iteration 800 / 1500: loss 7.979111\n",
      "iteration 900 / 1500: loss 6.072230\n",
      "iteration 1000 / 1500: loss 4.612009\n",
      "iteration 1100 / 1500: loss 3.762262\n",
      "iteration 1200 / 1500: loss 3.155498\n",
      "iteration 1300 / 1500: loss 2.847724\n",
      "iteration 1400 / 1500: loss 2.400267\n",
      "iteration 0 / 1500: loss 67.351150\n",
      "iteration 100 / 1500: loss 55.801440\n",
      "iteration 200 / 1500: loss 47.195633\n",
      "iteration 300 / 1500: loss 40.352704\n",
      "iteration 400 / 1500: loss 34.546306\n",
      "iteration 500 / 1500: loss 29.760317\n",
      "iteration 600 / 1500: loss 25.336364\n",
      "iteration 700 / 1500: loss 21.747838\n",
      "iteration 800 / 1500: loss 18.730237\n",
      "iteration 900 / 1500: loss 16.183037\n",
      "iteration 1000 / 1500: loss 14.244789\n",
      "iteration 1100 / 1500: loss 12.280505\n",
      "iteration 1200 / 1500: loss 10.709082\n",
      "iteration 1300 / 1500: loss 9.415151\n",
      "iteration 1400 / 1500: loss 8.278146\n",
      "iteration 0 / 1500: loss 35.913377\n",
      "iteration 100 / 1500: loss 31.628615\n",
      "iteration 200 / 1500: loss 28.616456\n",
      "iteration 300 / 1500: loss 26.612779\n",
      "iteration 400 / 1500: loss 24.477991\n",
      "iteration 500 / 1500: loss 22.650237\n",
      "iteration 600 / 1500: loss 20.874807\n",
      "iteration 700 / 1500: loss 19.323890\n",
      "iteration 800 / 1500: loss 17.702199\n",
      "iteration 900 / 1500: loss 16.692218\n",
      "iteration 1000 / 1500: loss 15.330623\n",
      "iteration 1100 / 1500: loss 14.450743\n",
      "iteration 1200 / 1500: loss 13.515660\n",
      "iteration 1300 / 1500: loss 12.569437\n",
      "iteration 1400 / 1500: loss 11.707974\n",
      "iteration 0 / 1500: loss 313.518764\n",
      "iteration 100 / 1500: loss 141.017730\n",
      "iteration 200 / 1500: loss 63.977121\n",
      "iteration 300 / 1500: loss 29.710086\n",
      "iteration 400 / 1500: loss 14.429998\n",
      "iteration 500 / 1500: loss 7.523030\n",
      "iteration 600 / 1500: loss 4.475077\n",
      "iteration 700 / 1500: loss 3.116817\n",
      "iteration 800 / 1500: loss 2.526840\n",
      "iteration 900 / 1500: loss 2.219990\n",
      "iteration 1000 / 1500: loss 2.182796\n",
      "iteration 1100 / 1500: loss 1.996691\n",
      "iteration 1200 / 1500: loss 2.011922\n",
      "iteration 1300 / 1500: loss 1.966482\n",
      "iteration 1400 / 1500: loss 2.058370\n",
      "iteration 0 / 1500: loss 627.708018\n",
      "iteration 100 / 1500: loss 126.407513\n",
      "iteration 200 / 1500: loss 26.924027\n",
      "iteration 300 / 1500: loss 7.037344\n",
      "iteration 400 / 1500: loss 3.041124\n",
      "iteration 500 / 1500: loss 2.239703\n",
      "iteration 600 / 1500: loss 2.153969\n",
      "iteration 700 / 1500: loss 2.069339\n",
      "iteration 800 / 1500: loss 2.007388\n",
      "iteration 900 / 1500: loss 2.026104\n",
      "iteration 1000 / 1500: loss 2.052822\n",
      "iteration 1100 / 1500: loss 2.086850\n",
      "iteration 1200 / 1500: loss 2.080587\n",
      "iteration 1300 / 1500: loss 2.094210\n",
      "iteration 1400 / 1500: loss 2.016846\n",
      "iteration 0 / 1500: loss 1552.703336\n",
      "iteration 100 / 1500: loss 29.138886\n",
      "iteration 200 / 1500: loss 2.614132\n",
      "iteration 300 / 1500: loss 2.161159\n",
      "iteration 400 / 1500: loss 2.175594\n",
      "iteration 500 / 1500: loss 2.183297\n",
      "iteration 600 / 1500: loss 2.121328\n",
      "iteration 700 / 1500: loss 2.163560\n",
      "iteration 800 / 1500: loss 2.142091\n",
      "iteration 900 / 1500: loss 2.146990\n",
      "iteration 1000 / 1500: loss 2.175762\n",
      "iteration 1100 / 1500: loss 2.171934\n",
      "iteration 1200 / 1500: loss 2.158538\n",
      "iteration 1300 / 1500: loss 2.158202\n",
      "iteration 1400 / 1500: loss 2.109986\n",
      "iteration 0 / 1500: loss 3037.322075\n",
      "iteration 100 / 1500: loss 3.024601\n",
      "iteration 200 / 1500: loss 2.220479\n",
      "iteration 300 / 1500: loss 2.204766\n",
      "iteration 400 / 1500: loss 2.199146\n",
      "iteration 500 / 1500: loss 2.178611\n",
      "iteration 600 / 1500: loss 2.224738\n",
      "iteration 700 / 1500: loss 2.200288\n",
      "iteration 800 / 1500: loss 2.178156\n",
      "iteration 900 / 1500: loss 2.204565\n",
      "iteration 1000 / 1500: loss 2.226704\n",
      "iteration 1100 / 1500: loss 2.164149\n",
      "iteration 1200 / 1500: loss 2.179177\n",
      "iteration 1300 / 1500: loss 2.202443\n",
      "iteration 1400 / 1500: loss 2.206494\n",
      "iteration 0 / 1500: loss 6109.240945\n",
      "iteration 100 / 1500: loss 2.223928\n",
      "iteration 200 / 1500: loss 2.244015\n",
      "iteration 300 / 1500: loss 2.239448\n",
      "iteration 400 / 1500: loss 2.242273\n",
      "iteration 500 / 1500: loss 2.225519\n",
      "iteration 600 / 1500: loss 2.234524\n",
      "iteration 700 / 1500: loss 2.243316\n",
      "iteration 800 / 1500: loss 2.220068\n",
      "iteration 900 / 1500: loss 2.239584\n",
      "iteration 1000 / 1500: loss 2.236347\n",
      "iteration 1100 / 1500: loss 2.229525\n",
      "iteration 1200 / 1500: loss 2.237520\n",
      "iteration 1300 / 1500: loss 2.249022\n",
      "iteration 1400 / 1500: loss 2.208417\n",
      "iteration 0 / 1500: loss 158.476416\n",
      "iteration 100 / 1500: loss 85.893646\n",
      "iteration 200 / 1500: loss 47.621726\n",
      "iteration 300 / 1500: loss 26.935955\n",
      "iteration 400 / 1500: loss 15.534488\n",
      "iteration 500 / 1500: loss 9.436442\n",
      "iteration 600 / 1500: loss 6.030480\n",
      "iteration 700 / 1500: loss 4.199119\n",
      "iteration 800 / 1500: loss 3.120173\n",
      "iteration 900 / 1500: loss 2.578082\n",
      "iteration 1000 / 1500: loss 2.324707\n",
      "iteration 1100 / 1500: loss 2.089958\n",
      "iteration 1200 / 1500: loss 2.044361\n",
      "iteration 1300 / 1500: loss 1.997408\n",
      "iteration 1400 / 1500: loss 1.946997\n",
      "iteration 0 / 1500: loss 67.295006\n",
      "iteration 100 / 1500: loss 50.650922\n",
      "iteration 200 / 1500: loss 40.046464\n",
      "iteration 300 / 1500: loss 31.624755\n",
      "iteration 400 / 1500: loss 25.150887\n",
      "iteration 500 / 1500: loss 20.131816\n",
      "iteration 600 / 1500: loss 16.146158\n",
      "iteration 700 / 1500: loss 13.098402\n",
      "iteration 800 / 1500: loss 10.663357\n",
      "iteration 900 / 1500: loss 8.655850\n",
      "iteration 1000 / 1500: loss 7.255664\n",
      "iteration 1100 / 1500: loss 5.991647\n",
      "iteration 1200 / 1500: loss 5.115652\n",
      "iteration 1300 / 1500: loss 4.433215\n",
      "iteration 1400 / 1500: loss 3.901063\n",
      "iteration 0 / 1500: loss 35.545428\n",
      "iteration 100 / 1500: loss 30.421296\n",
      "iteration 200 / 1500: loss 26.742688\n",
      "iteration 300 / 1500: loss 23.786898\n",
      "iteration 400 / 1500: loss 20.935210\n",
      "iteration 500 / 1500: loss 18.680993\n",
      "iteration 600 / 1500: loss 16.766362\n",
      "iteration 700 / 1500: loss 15.051058\n",
      "iteration 800 / 1500: loss 13.698153\n",
      "iteration 900 / 1500: loss 12.070129\n",
      "iteration 1000 / 1500: loss 10.870545\n",
      "iteration 1100 / 1500: loss 9.873880\n",
      "iteration 1200 / 1500: loss 8.876670\n",
      "iteration 1300 / 1500: loss 7.958837\n",
      "iteration 1400 / 1500: loss 7.530014\n",
      "iteration 0 / 1500: loss 311.941776\n",
      "iteration 100 / 1500: loss 93.859656\n",
      "iteration 200 / 1500: loss 29.430265\n",
      "iteration 300 / 1500: loss 10.184805\n",
      "iteration 400 / 1500: loss 4.453945\n",
      "iteration 500 / 1500: loss 2.775888\n",
      "iteration 600 / 1500: loss 2.203134\n",
      "iteration 700 / 1500: loss 1.982731\n",
      "iteration 800 / 1500: loss 2.010723\n",
      "iteration 900 / 1500: loss 1.965312\n",
      "iteration 1000 / 1500: loss 2.024556\n",
      "iteration 1100 / 1500: loss 1.990733\n",
      "iteration 1200 / 1500: loss 1.942066\n",
      "iteration 1300 / 1500: loss 2.076963\n",
      "iteration 1400 / 1500: loss 2.034891\n",
      "iteration 0 / 1500: loss 622.212301\n",
      "iteration 100 / 1500: loss 56.950766\n",
      "iteration 200 / 1500: loss 6.966774\n",
      "iteration 300 / 1500: loss 2.587924\n",
      "iteration 400 / 1500: loss 2.103095\n",
      "iteration 500 / 1500: loss 2.031809\n",
      "iteration 600 / 1500: loss 2.070412\n",
      "iteration 700 / 1500: loss 2.011791\n",
      "iteration 800 / 1500: loss 2.047700\n",
      "iteration 900 / 1500: loss 2.005002\n",
      "iteration 1000 / 1500: loss 1.957613\n",
      "iteration 1100 / 1500: loss 2.020004\n",
      "iteration 1200 / 1500: loss 2.091694\n",
      "iteration 1300 / 1500: loss 2.160239\n",
      "iteration 1400 / 1500: loss 2.102341\n",
      "iteration 0 / 1500: loss 1530.439293\n",
      "iteration 100 / 1500: loss 5.564940\n",
      "iteration 200 / 1500: loss 2.156965\n",
      "iteration 300 / 1500: loss 2.142876\n",
      "iteration 400 / 1500: loss 2.144774\n",
      "iteration 500 / 1500: loss 2.130819\n",
      "iteration 600 / 1500: loss 2.131445\n",
      "iteration 700 / 1500: loss 2.177703\n",
      "iteration 800 / 1500: loss 2.137553\n",
      "iteration 900 / 1500: loss 2.172101\n",
      "iteration 1000 / 1500: loss 2.165747\n",
      "iteration 1100 / 1500: loss 2.132501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.130485\n",
      "iteration 1300 / 1500: loss 2.205446\n",
      "iteration 1400 / 1500: loss 2.169563\n",
      "iteration 0 / 1500: loss 3143.199098\n",
      "iteration 100 / 1500: loss 2.220190\n",
      "iteration 200 / 1500: loss 2.224033\n",
      "iteration 300 / 1500: loss 2.182889\n",
      "iteration 400 / 1500: loss 2.160399\n",
      "iteration 500 / 1500: loss 2.191444\n",
      "iteration 600 / 1500: loss 2.185844\n",
      "iteration 700 / 1500: loss 2.169968\n",
      "iteration 800 / 1500: loss 2.205846\n",
      "iteration 900 / 1500: loss 2.212134\n",
      "iteration 1000 / 1500: loss 2.142329\n",
      "iteration 1100 / 1500: loss 2.194174\n",
      "iteration 1200 / 1500: loss 2.183822\n",
      "iteration 1300 / 1500: loss 2.181807\n",
      "iteration 1400 / 1500: loss 2.201606\n",
      "iteration 0 / 1500: loss 6073.609433\n",
      "iteration 100 / 1500: loss 2.230110\n",
      "iteration 200 / 1500: loss 2.261449\n",
      "iteration 300 / 1500: loss 2.224624\n",
      "iteration 400 / 1500: loss 2.216534\n",
      "iteration 500 / 1500: loss 2.241854\n",
      "iteration 600 / 1500: loss 2.254327\n",
      "iteration 700 / 1500: loss 2.227142\n",
      "iteration 800 / 1500: loss 2.241338\n",
      "iteration 900 / 1500: loss 2.240443\n",
      "iteration 1000 / 1500: loss 2.235721\n",
      "iteration 1100 / 1500: loss 2.235608\n",
      "iteration 1200 / 1500: loss 2.257604\n",
      "iteration 1300 / 1500: loss 2.243558\n",
      "iteration 1400 / 1500: loss 2.258156\n",
      "iteration 0 / 1500: loss 158.624823\n",
      "iteration 100 / 1500: loss 58.401454\n",
      "iteration 200 / 1500: loss 22.461055\n",
      "iteration 300 / 1500: loss 9.468564\n",
      "iteration 400 / 1500: loss 4.678453\n",
      "iteration 500 / 1500: loss 2.926467\n",
      "iteration 600 / 1500: loss 2.223895\n",
      "iteration 700 / 1500: loss 1.975056\n",
      "iteration 800 / 1500: loss 1.893976\n",
      "iteration 900 / 1500: loss 1.972446\n",
      "iteration 1000 / 1500: loss 2.004218\n",
      "iteration 1100 / 1500: loss 1.968189\n",
      "iteration 1200 / 1500: loss 1.974224\n",
      "iteration 1300 / 1500: loss 1.957117\n",
      "iteration 1400 / 1500: loss 2.073768\n",
      "iteration 0 / 1500: loss 66.750649\n",
      "iteration 100 / 1500: loss 43.137213\n",
      "iteration 200 / 1500: loss 29.211097\n",
      "iteration 300 / 1500: loss 20.102923\n",
      "iteration 400 / 1500: loss 13.875533\n",
      "iteration 500 / 1500: loss 9.951114\n",
      "iteration 600 / 1500: loss 7.212073\n",
      "iteration 700 / 1500: loss 5.476830\n",
      "iteration 800 / 1500: loss 4.279555\n",
      "iteration 900 / 1500: loss 3.494517\n",
      "iteration 1000 / 1500: loss 2.969409\n",
      "iteration 1100 / 1500: loss 2.639080\n",
      "iteration 1200 / 1500: loss 2.296328\n",
      "iteration 1300 / 1500: loss 2.166135\n",
      "iteration 1400 / 1500: loss 2.119808\n",
      "iteration 0 / 1500: loss 36.496967\n",
      "iteration 100 / 1500: loss 28.106288\n",
      "iteration 200 / 1500: loss 22.935903\n",
      "iteration 300 / 1500: loss 18.881219\n",
      "iteration 400 / 1500: loss 15.575712\n",
      "iteration 500 / 1500: loss 12.969417\n",
      "iteration 600 / 1500: loss 10.918322\n",
      "iteration 700 / 1500: loss 9.249868\n",
      "iteration 800 / 1500: loss 7.991458\n",
      "iteration 900 / 1500: loss 6.791597\n",
      "iteration 1000 / 1500: loss 5.758243\n",
      "iteration 1100 / 1500: loss 5.031054\n",
      "iteration 1200 / 1500: loss 4.485232\n",
      "iteration 1300 / 1500: loss 4.073124\n",
      "iteration 1400 / 1500: loss 3.644127\n",
      "iteration 0 / 1500: loss 311.136691\n",
      "iteration 100 / 1500: loss 42.642048\n",
      "iteration 200 / 1500: loss 7.415325\n",
      "iteration 300 / 1500: loss 2.692546\n",
      "iteration 400 / 1500: loss 2.090654\n",
      "iteration 500 / 1500: loss 2.053145\n",
      "iteration 600 / 1500: loss 1.970231\n",
      "iteration 700 / 1500: loss 1.978973\n",
      "iteration 800 / 1500: loss 2.075878\n",
      "iteration 900 / 1500: loss 2.054273\n",
      "iteration 1000 / 1500: loss 1.979299\n",
      "iteration 1100 / 1500: loss 2.018001\n",
      "iteration 1200 / 1500: loss 2.060484\n",
      "iteration 1300 / 1500: loss 1.977008\n",
      "iteration 1400 / 1500: loss 2.129401\n",
      "iteration 0 / 1500: loss 630.811073\n",
      "iteration 100 / 1500: loss 12.922450\n",
      "iteration 200 / 1500: loss 2.242999\n",
      "iteration 300 / 1500: loss 2.077248\n",
      "iteration 400 / 1500: loss 2.087276\n",
      "iteration 500 / 1500: loss 2.071057\n",
      "iteration 600 / 1500: loss 2.080929\n",
      "iteration 700 / 1500: loss 2.111940\n",
      "iteration 800 / 1500: loss 2.072230\n",
      "iteration 900 / 1500: loss 2.078094\n",
      "iteration 1000 / 1500: loss 2.052125\n",
      "iteration 1100 / 1500: loss 2.099640\n",
      "iteration 1200 / 1500: loss 2.009233\n",
      "iteration 1300 / 1500: loss 2.075247\n",
      "iteration 1400 / 1500: loss 2.068567\n",
      "iteration 0 / 1500: loss 1524.088037\n",
      "iteration 100 / 1500: loss 2.187700\n",
      "iteration 200 / 1500: loss 2.150578\n",
      "iteration 300 / 1500: loss 2.182861\n",
      "iteration 400 / 1500: loss 2.143479\n",
      "iteration 500 / 1500: loss 2.108145\n",
      "iteration 600 / 1500: loss 2.139243\n",
      "iteration 700 / 1500: loss 2.147385\n",
      "iteration 800 / 1500: loss 2.196476\n",
      "iteration 900 / 1500: loss 2.122387\n",
      "iteration 1000 / 1500: loss 2.159862\n",
      "iteration 1100 / 1500: loss 2.157519\n",
      "iteration 1200 / 1500: loss 2.115011\n",
      "iteration 1300 / 1500: loss 2.154058\n",
      "iteration 1400 / 1500: loss 2.146881\n",
      "iteration 0 / 1500: loss 3087.777691\n",
      "iteration 100 / 1500: loss 2.174130\n",
      "iteration 200 / 1500: loss 2.198006\n",
      "iteration 300 / 1500: loss 2.167292\n",
      "iteration 400 / 1500: loss 2.215143\n",
      "iteration 500 / 1500: loss 2.198747\n",
      "iteration 600 / 1500: loss 2.139119\n",
      "iteration 700 / 1500: loss 2.213082\n",
      "iteration 800 / 1500: loss 2.185477\n",
      "iteration 900 / 1500: loss 2.186766\n",
      "iteration 1000 / 1500: loss 2.227817\n",
      "iteration 1100 / 1500: loss 2.201171\n",
      "iteration 1200 / 1500: loss 2.206910\n",
      "iteration 1300 / 1500: loss 2.232955\n",
      "iteration 1400 / 1500: loss 2.204453\n",
      "iteration 0 / 1500: loss 6171.143599\n",
      "iteration 100 / 1500: loss 2.232824\n",
      "iteration 200 / 1500: loss 2.243194\n",
      "iteration 300 / 1500: loss 2.247887\n",
      "iteration 400 / 1500: loss 2.251921\n",
      "iteration 500 / 1500: loss 2.235096\n",
      "iteration 600 / 1500: loss 2.249225\n",
      "iteration 700 / 1500: loss 2.236893\n",
      "iteration 800 / 1500: loss 2.246249\n",
      "iteration 900 / 1500: loss 2.217057\n",
      "iteration 1000 / 1500: loss 2.243312\n",
      "iteration 1100 / 1500: loss 2.256681\n",
      "iteration 1200 / 1500: loss 2.232258\n",
      "iteration 1300 / 1500: loss 2.232992\n",
      "iteration 1400 / 1500: loss 2.240061\n",
      "iteration 0 / 1500: loss 159.453731\n",
      "iteration 100 / 1500: loss 155.692186\n",
      "iteration 200 / 1500: loss 152.445927\n",
      "iteration 300 / 1500: loss 149.416320\n",
      "iteration 400 / 1500: loss 145.992114\n",
      "iteration 500 / 1500: loss 143.578910\n",
      "iteration 600 / 1500: loss 140.393427\n",
      "iteration 700 / 1500: loss 137.625694\n",
      "iteration 800 / 1500: loss 134.363957\n",
      "iteration 900 / 1500: loss 132.132987\n",
      "iteration 1000 / 1500: loss 129.106745\n",
      "iteration 1100 / 1500: loss 126.555463\n",
      "iteration 1200 / 1500: loss 124.118717\n",
      "iteration 1300 / 1500: loss 121.611309\n",
      "iteration 1400 / 1500: loss 119.410879\n",
      "iteration 0 / 1500: loss 68.984969\n",
      "iteration 100 / 1500: loss 67.632538\n",
      "iteration 200 / 1500: loss 66.937732\n",
      "iteration 300 / 1500: loss 65.728473\n",
      "iteration 400 / 1500: loss 64.955896\n",
      "iteration 500 / 1500: loss 64.363876\n",
      "iteration 600 / 1500: loss 63.759641\n",
      "iteration 700 / 1500: loss 63.138513\n",
      "iteration 800 / 1500: loss 62.507605\n",
      "iteration 900 / 1500: loss 61.951879\n",
      "iteration 1000 / 1500: loss 61.312988\n",
      "iteration 1100 / 1500: loss 60.824349\n",
      "iteration 1200 / 1500: loss 60.104015\n",
      "iteration 1300 / 1500: loss 60.083583\n",
      "iteration 1400 / 1500: loss 59.166725\n",
      "iteration 0 / 1500: loss 36.103461\n",
      "iteration 100 / 1500: loss 35.965409\n",
      "iteration 200 / 1500: loss 34.948322\n",
      "iteration 300 / 1500: loss 34.634351\n",
      "iteration 400 / 1500: loss 34.686699\n",
      "iteration 500 / 1500: loss 34.348754\n",
      "iteration 600 / 1500: loss 34.535316\n",
      "iteration 700 / 1500: loss 33.879078\n",
      "iteration 800 / 1500: loss 33.475577\n",
      "iteration 900 / 1500: loss 33.393169\n",
      "iteration 1000 / 1500: loss 33.310459\n",
      "iteration 1100 / 1500: loss 33.219272\n",
      "iteration 1200 / 1500: loss 32.835322\n",
      "iteration 1300 / 1500: loss 32.804793\n",
      "iteration 1400 / 1500: loss 32.627092\n",
      "iteration 0 / 1500: loss 315.246233\n",
      "iteration 100 / 1500: loss 302.816266\n",
      "iteration 200 / 1500: loss 291.042286\n",
      "iteration 300 / 1500: loss 279.267916\n",
      "iteration 400 / 1500: loss 268.298650\n",
      "iteration 500 / 1500: loss 257.811285\n",
      "iteration 600 / 1500: loss 247.789970\n",
      "iteration 700 / 1500: loss 238.036609\n",
      "iteration 800 / 1500: loss 228.665743\n",
      "iteration 900 / 1500: loss 219.259355\n",
      "iteration 1000 / 1500: loss 210.939990\n",
      "iteration 1100 / 1500: loss 202.622020\n",
      "iteration 1200 / 1500: loss 194.530270\n",
      "iteration 1300 / 1500: loss 186.793161\n",
      "iteration 1400 / 1500: loss 179.644818\n",
      "iteration 0 / 1500: loss 623.788226\n",
      "iteration 100 / 1500: loss 575.964290\n",
      "iteration 200 / 1500: loss 530.792307\n",
      "iteration 300 / 1500: loss 490.363153\n",
      "iteration 400 / 1500: loss 452.414954\n",
      "iteration 500 / 1500: loss 417.682970\n",
      "iteration 600 / 1500: loss 385.241164\n",
      "iteration 700 / 1500: loss 355.748041\n",
      "iteration 800 / 1500: loss 328.309110\n",
      "iteration 900 / 1500: loss 303.348238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 279.949976\n",
      "iteration 1100 / 1500: loss 258.829816\n",
      "iteration 1200 / 1500: loss 238.734465\n",
      "iteration 1300 / 1500: loss 220.514443\n",
      "iteration 1400 / 1500: loss 203.675796\n",
      "iteration 0 / 1500: loss 1551.077637\n",
      "iteration 100 / 1500: loss 1269.291841\n",
      "iteration 200 / 1500: loss 1039.326847\n",
      "iteration 300 / 1500: loss 850.753859\n",
      "iteration 400 / 1500: loss 696.703295\n",
      "iteration 500 / 1500: loss 570.518043\n",
      "iteration 600 / 1500: loss 467.233464\n",
      "iteration 700 / 1500: loss 382.556786\n",
      "iteration 800 / 1500: loss 313.646998\n",
      "iteration 900 / 1500: loss 256.889481\n",
      "iteration 1000 / 1500: loss 210.666524\n",
      "iteration 1100 / 1500: loss 172.908729\n",
      "iteration 1200 / 1500: loss 141.858528\n",
      "iteration 1300 / 1500: loss 116.521157\n",
      "iteration 1400 / 1500: loss 95.708585\n",
      "iteration 0 / 1500: loss 3114.044630\n",
      "iteration 100 / 1500: loss 2086.584912\n",
      "iteration 200 / 1500: loss 1398.501986\n",
      "iteration 300 / 1500: loss 937.483417\n",
      "iteration 400 / 1500: loss 628.697839\n",
      "iteration 500 / 1500: loss 421.833105\n",
      "iteration 600 / 1500: loss 283.229376\n",
      "iteration 700 / 1500: loss 190.367117\n",
      "iteration 800 / 1500: loss 128.253857\n",
      "iteration 900 / 1500: loss 86.614793\n",
      "iteration 1000 / 1500: loss 58.784748\n",
      "iteration 1100 / 1500: loss 40.135904\n",
      "iteration 1200 / 1500: loss 27.565311\n",
      "iteration 1300 / 1500: loss 19.204042\n",
      "iteration 1400 / 1500: loss 13.583630\n",
      "iteration 0 / 1500: loss 6269.327004\n",
      "iteration 100 / 1500: loss 2813.109770\n",
      "iteration 200 / 1500: loss 1262.573094\n",
      "iteration 300 / 1500: loss 567.204813\n",
      "iteration 400 / 1500: loss 255.567089\n",
      "iteration 500 / 1500: loss 115.791675\n",
      "iteration 600 / 1500: loss 53.149939\n",
      "iteration 700 / 1500: loss 25.069712\n",
      "iteration 800 / 1500: loss 12.489117\n",
      "iteration 900 / 1500: loss 6.831774\n",
      "iteration 1000 / 1500: loss 4.283490\n",
      "iteration 1100 / 1500: loss 3.132718\n",
      "iteration 1200 / 1500: loss 2.633753\n",
      "iteration 1300 / 1500: loss 2.414346\n",
      "iteration 1400 / 1500: loss 2.284753\n",
      "iteration 0 / 1500: loss 159.482042\n",
      "iteration 100 / 1500: loss 49.266083\n",
      "iteration 200 / 1500: loss 36.642658\n",
      "iteration 300 / 1500: loss 40.350761\n",
      "iteration 400 / 1500: loss 36.415967\n",
      "iteration 500 / 1500: loss 38.368465\n",
      "iteration 600 / 1500: loss 50.933355\n",
      "iteration 700 / 1500: loss 27.047764\n",
      "iteration 800 / 1500: loss 42.951315\n",
      "iteration 900 / 1500: loss 38.129393\n",
      "iteration 1000 / 1500: loss 41.919843\n",
      "iteration 1100 / 1500: loss 28.191895\n",
      "iteration 1200 / 1500: loss 31.495015\n",
      "iteration 1300 / 1500: loss 52.940969\n",
      "iteration 1400 / 1500: loss 29.430557\n",
      "iteration 0 / 1500: loss 67.968919\n",
      "iteration 100 / 1500: loss 28.150472\n",
      "iteration 200 / 1500: loss 22.977474\n",
      "iteration 300 / 1500: loss 24.104578\n",
      "iteration 400 / 1500: loss 22.136724\n",
      "iteration 500 / 1500: loss 22.577573\n",
      "iteration 600 / 1500: loss 26.593689\n",
      "iteration 700 / 1500: loss 33.735237\n",
      "iteration 800 / 1500: loss 29.816535\n",
      "iteration 900 / 1500: loss 23.283753\n",
      "iteration 1000 / 1500: loss 33.993213\n",
      "iteration 1100 / 1500: loss 23.362692\n",
      "iteration 1200 / 1500: loss 23.749075\n",
      "iteration 1300 / 1500: loss 40.174612\n",
      "iteration 1400 / 1500: loss 27.680556\n",
      "iteration 0 / 1500: loss 35.754268\n",
      "iteration 100 / 1500: loss 17.281074\n",
      "iteration 200 / 1500: loss 16.906069\n",
      "iteration 300 / 1500: loss 23.650138\n",
      "iteration 400 / 1500: loss 23.052762\n",
      "iteration 500 / 1500: loss 18.591079\n",
      "iteration 600 / 1500: loss 19.184941\n",
      "iteration 700 / 1500: loss 23.848395\n",
      "iteration 800 / 1500: loss 23.450180\n",
      "iteration 900 / 1500: loss 19.077292\n",
      "iteration 1000 / 1500: loss 20.122277\n",
      "iteration 1100 / 1500: loss 15.050830\n",
      "iteration 1200 / 1500: loss 26.480468\n",
      "iteration 1300 / 1500: loss 21.123606\n",
      "iteration 1400 / 1500: loss 24.648294\n",
      "iteration 0 / 1500: loss 316.174989\n",
      "iteration 100 / 1500: loss 86.872687\n",
      "iteration 200 / 1500: loss 80.088054\n",
      "iteration 300 / 1500: loss 95.180891\n",
      "iteration 400 / 1500: loss 75.917946\n",
      "iteration 500 / 1500: loss 83.868982\n",
      "iteration 600 / 1500: loss 87.219237\n",
      "iteration 700 / 1500: loss 99.361437\n",
      "iteration 800 / 1500: loss 102.610608\n",
      "iteration 900 / 1500: loss 88.082929\n",
      "iteration 1000 / 1500: loss 74.225411\n",
      "iteration 1100 / 1500: loss 85.951237\n",
      "iteration 1200 / 1500: loss 83.242751\n",
      "iteration 1300 / 1500: loss 77.974255\n",
      "iteration 1400 / 1500: loss 86.648038\n",
      "iteration 0 / 1500: loss 629.082854\n",
      "iteration 100 / 1500: loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yagiz\\Desktop\\StanfordCV\\Assignment1\\assignment1\\cs231n\\classifiers\\softmax.py:92: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_vector = C*np.log(correct_class_scores)/-C\n",
      "C:\\Users\\Yagiz\\Desktop\\StanfordCV\\Assignment1\\assignment1\\cs231n\\classifiers\\softmax.py:87: RuntimeWarning: overflow encountered in exp\n",
      "  scores = np.exp(scores)##Scores exponential\n",
      "C:\\Users\\Yagiz\\Desktop\\StanfordCV\\Assignment1\\assignment1\\cs231n\\classifiers\\softmax.py:89: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_scores = scores/sum_of_scores[:,None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1540.504533\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 3059.002132\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6223.475816\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 155.287238\n",
      "iteration 100 / 1500: loss 21.943696\n",
      "iteration 200 / 1500: loss 4.481167\n",
      "iteration 300 / 1500: loss 2.340416\n",
      "iteration 400 / 1500: loss 1.981316\n",
      "iteration 500 / 1500: loss 1.967566\n",
      "iteration 600 / 1500: loss 1.865873\n",
      "iteration 700 / 1500: loss 1.961065\n",
      "iteration 800 / 1500: loss 1.929661\n",
      "iteration 900 / 1500: loss 1.986190\n",
      "iteration 1000 / 1500: loss 1.996677\n",
      "iteration 1100 / 1500: loss 1.998002\n",
      "iteration 1200 / 1500: loss 1.969141\n",
      "iteration 1300 / 1500: loss 1.967892\n",
      "iteration 1400 / 1500: loss 1.848432\n",
      "iteration 0 / 1500: loss 66.919849\n",
      "iteration 100 / 1500: loss 29.312497\n",
      "iteration 200 / 1500: loss 14.061411\n",
      "iteration 300 / 1500: loss 7.262195\n",
      "iteration 400 / 1500: loss 4.275093\n",
      "iteration 500 / 1500: loss 2.917941\n",
      "iteration 600 / 1500: loss 2.332516\n",
      "iteration 700 / 1500: loss 2.046974\n",
      "iteration 800 / 1500: loss 2.072825\n",
      "iteration 900 / 1500: loss 2.006917\n",
      "iteration 1000 / 1500: loss 1.821727\n",
      "iteration 1100 / 1500: loss 1.950346\n",
      "iteration 1200 / 1500: loss 1.809950\n",
      "iteration 1300 / 1500: loss 1.970942\n",
      "iteration 1400 / 1500: loss 1.954527\n",
      "iteration 0 / 1500: loss 35.978820\n",
      "iteration 100 / 1500: loss 22.944396\n",
      "iteration 200 / 1500: loss 15.727315\n",
      "iteration 300 / 1500: loss 10.957249\n",
      "iteration 400 / 1500: loss 7.860318\n",
      "iteration 500 / 1500: loss 5.901493\n",
      "iteration 600 / 1500: loss 4.426166\n",
      "iteration 700 / 1500: loss 3.600050\n",
      "iteration 800 / 1500: loss 2.983484\n",
      "iteration 900 / 1500: loss 2.577009\n",
      "iteration 1000 / 1500: loss 2.352385\n",
      "iteration 1100 / 1500: loss 2.237787\n",
      "iteration 1200 / 1500: loss 2.123650\n",
      "iteration 1300 / 1500: loss 2.047386\n",
      "iteration 1400 / 1500: loss 2.112366\n",
      "iteration 0 / 1500: loss 314.258170\n",
      "iteration 100 / 1500: loss 7.288405\n",
      "iteration 200 / 1500: loss 2.102798\n",
      "iteration 300 / 1500: loss 1.979343\n",
      "iteration 400 / 1500: loss 2.028191\n",
      "iteration 500 / 1500: loss 2.002326\n",
      "iteration 600 / 1500: loss 2.027548\n",
      "iteration 700 / 1500: loss 1.931172\n",
      "iteration 800 / 1500: loss 2.006749\n",
      "iteration 900 / 1500: loss 1.996673\n",
      "iteration 1000 / 1500: loss 1.960800\n",
      "iteration 1100 / 1500: loss 1.969643\n",
      "iteration 1200 / 1500: loss 1.998477\n",
      "iteration 1300 / 1500: loss 2.009350\n",
      "iteration 1400 / 1500: loss 2.018625\n",
      "iteration 0 / 1500: loss 620.466164\n",
      "iteration 100 / 1500: loss 2.348512\n",
      "iteration 200 / 1500: loss 2.023963\n",
      "iteration 300 / 1500: loss 2.153806\n",
      "iteration 400 / 1500: loss 2.084294\n",
      "iteration 500 / 1500: loss 2.024597\n",
      "iteration 600 / 1500: loss 2.010447\n",
      "iteration 700 / 1500: loss 2.046918\n",
      "iteration 800 / 1500: loss 2.130641\n",
      "iteration 900 / 1500: loss 2.052712\n",
      "iteration 1000 / 1500: loss 2.042536\n",
      "iteration 1100 / 1500: loss 2.043970\n",
      "iteration 1200 / 1500: loss 2.079129\n",
      "iteration 1300 / 1500: loss 2.077367\n",
      "iteration 1400 / 1500: loss 2.125283\n",
      "iteration 0 / 1500: loss 1552.429646\n",
      "iteration 100 / 1500: loss 2.162625\n",
      "iteration 200 / 1500: loss 2.130260\n",
      "iteration 300 / 1500: loss 2.140695\n",
      "iteration 400 / 1500: loss 2.129938\n",
      "iteration 500 / 1500: loss 2.159974\n",
      "iteration 600 / 1500: loss 2.122798\n",
      "iteration 700 / 1500: loss 2.218001\n",
      "iteration 800 / 1500: loss 2.175052\n",
      "iteration 900 / 1500: loss 2.178807\n",
      "iteration 1000 / 1500: loss 2.134197\n",
      "iteration 1100 / 1500: loss 2.154119\n",
      "iteration 1200 / 1500: loss 2.182353\n",
      "iteration 1300 / 1500: loss 2.144152\n",
      "iteration 1400 / 1500: loss 2.126362\n",
      "iteration 0 / 1500: loss 3054.723049\n",
      "iteration 100 / 1500: loss 2.216749\n",
      "iteration 200 / 1500: loss 2.204044\n",
      "iteration 300 / 1500: loss 2.191444\n",
      "iteration 400 / 1500: loss 2.178020\n",
      "iteration 500 / 1500: loss 2.246978\n",
      "iteration 600 / 1500: loss 2.194341\n",
      "iteration 700 / 1500: loss 2.170247\n",
      "iteration 800 / 1500: loss 2.207559\n",
      "iteration 900 / 1500: loss 2.185163\n",
      "iteration 1000 / 1500: loss 2.200816\n",
      "iteration 1100 / 1500: loss 2.219509\n",
      "iteration 1200 / 1500: loss 2.217810\n",
      "iteration 1300 / 1500: loss 2.211587\n",
      "iteration 1400 / 1500: loss 2.212646\n",
      "iteration 0 / 1500: loss 6121.132137\n",
      "iteration 100 / 1500: loss 2.230714\n",
      "iteration 200 / 1500: loss 2.244313\n",
      "iteration 300 / 1500: loss 2.235586\n",
      "iteration 400 / 1500: loss 2.248740\n",
      "iteration 500 / 1500: loss 2.257233\n",
      "iteration 600 / 1500: loss 2.238303\n",
      "iteration 700 / 1500: loss 2.263164\n",
      "iteration 800 / 1500: loss 2.267194\n",
      "iteration 900 / 1500: loss 2.260354\n",
      "iteration 1000 / 1500: loss 2.253339\n",
      "iteration 1100 / 1500: loss 2.262986\n",
      "iteration 1200 / 1500: loss 2.233101\n",
      "iteration 1300 / 1500: loss 2.259063\n",
      "iteration 1400 / 1500: loss 2.242989\n",
      "iteration 0 / 1500: loss 159.765911\n",
      "iteration 100 / 1500: loss 4.544595\n",
      "iteration 200 / 1500: loss 2.067784\n",
      "iteration 300 / 1500: loss 1.949393\n",
      "iteration 400 / 1500: loss 1.969830\n",
      "iteration 500 / 1500: loss 1.996872\n",
      "iteration 600 / 1500: loss 1.992114\n",
      "iteration 700 / 1500: loss 1.934867\n",
      "iteration 800 / 1500: loss 1.986704\n",
      "iteration 900 / 1500: loss 1.956581\n",
      "iteration 1000 / 1500: loss 1.997785\n",
      "iteration 1100 / 1500: loss 2.007513\n",
      "iteration 1200 / 1500: loss 2.008810\n",
      "iteration 1300 / 1500: loss 1.980489\n",
      "iteration 1400 / 1500: loss 1.995966\n",
      "iteration 0 / 1500: loss 66.196577\n",
      "iteration 100 / 1500: loss 14.042314\n",
      "iteration 200 / 1500: loss 4.171239\n",
      "iteration 300 / 1500: loss 2.306041\n",
      "iteration 400 / 1500: loss 1.892370\n",
      "iteration 500 / 1500: loss 1.954241\n",
      "iteration 600 / 1500: loss 1.854676\n",
      "iteration 700 / 1500: loss 1.908057\n",
      "iteration 800 / 1500: loss 1.898749\n",
      "iteration 900 / 1500: loss 1.900125\n",
      "iteration 1000 / 1500: loss 1.826739\n",
      "iteration 1100 / 1500: loss 2.006678\n",
      "iteration 1200 / 1500: loss 1.786430\n",
      "iteration 1300 / 1500: loss 1.875992\n",
      "iteration 1400 / 1500: loss 1.838426\n",
      "iteration 0 / 1500: loss 35.549818\n",
      "iteration 100 / 1500: loss 15.646757\n",
      "iteration 200 / 1500: loss 7.937094\n",
      "iteration 300 / 1500: loss 4.490836\n",
      "iteration 400 / 1500: loss 3.082391\n",
      "iteration 500 / 1500: loss 2.419212\n",
      "iteration 600 / 1500: loss 2.040471\n",
      "iteration 700 / 1500: loss 1.881201\n",
      "iteration 800 / 1500: loss 1.838663\n",
      "iteration 900 / 1500: loss 1.875806\n",
      "iteration 1000 / 1500: loss 1.914167\n",
      "iteration 1100 / 1500: loss 1.844655\n",
      "iteration 1200 / 1500: loss 1.910908\n",
      "iteration 1300 / 1500: loss 1.845070\n",
      "iteration 1400 / 1500: loss 1.852891\n",
      "iteration 0 / 1500: loss 313.703774\n",
      "iteration 100 / 1500: loss 2.046580\n",
      "iteration 200 / 1500: loss 1.990573\n",
      "iteration 300 / 1500: loss 2.023449\n",
      "iteration 400 / 1500: loss 2.018866\n",
      "iteration 500 / 1500: loss 2.087445\n",
      "iteration 600 / 1500: loss 2.053243\n",
      "iteration 700 / 1500: loss 1.994777\n",
      "iteration 800 / 1500: loss 2.043980\n",
      "iteration 900 / 1500: loss 1.991108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 1.945100\n",
      "iteration 1100 / 1500: loss 2.031982\n",
      "iteration 1200 / 1500: loss 2.051201\n",
      "iteration 1300 / 1500: loss 2.056842\n",
      "iteration 1400 / 1500: loss 1.952487\n",
      "iteration 0 / 1500: loss 630.132980\n",
      "iteration 100 / 1500: loss 2.101250\n",
      "iteration 200 / 1500: loss 2.200590\n",
      "iteration 300 / 1500: loss 2.157496\n",
      "iteration 400 / 1500: loss 2.084907\n",
      "iteration 500 / 1500: loss 2.100090\n",
      "iteration 600 / 1500: loss 2.113501\n",
      "iteration 700 / 1500: loss 2.024406\n",
      "iteration 800 / 1500: loss 2.118442\n",
      "iteration 900 / 1500: loss 2.124356\n",
      "iteration 1000 / 1500: loss 2.099539\n",
      "iteration 1100 / 1500: loss 2.082362\n",
      "iteration 1200 / 1500: loss 2.156836\n",
      "iteration 1300 / 1500: loss 2.061967\n",
      "iteration 1400 / 1500: loss 2.075417\n",
      "iteration 0 / 1500: loss 1534.176235\n",
      "iteration 100 / 1500: loss 2.126387\n",
      "iteration 200 / 1500: loss 2.178506\n",
      "iteration 300 / 1500: loss 2.169240\n",
      "iteration 400 / 1500: loss 2.119002\n",
      "iteration 500 / 1500: loss 2.150453\n",
      "iteration 600 / 1500: loss 2.199030\n",
      "iteration 700 / 1500: loss 2.135755\n",
      "iteration 800 / 1500: loss 2.191598\n",
      "iteration 900 / 1500: loss 2.180674\n",
      "iteration 1000 / 1500: loss 2.155026\n",
      "iteration 1100 / 1500: loss 2.189632\n",
      "iteration 1200 / 1500: loss 2.189357\n",
      "iteration 1300 / 1500: loss 2.203120\n",
      "iteration 1400 / 1500: loss 2.200957\n",
      "iteration 0 / 1500: loss 3060.061991\n",
      "iteration 100 / 1500: loss 2.260945\n",
      "iteration 200 / 1500: loss 2.247897\n",
      "iteration 300 / 1500: loss 2.217010\n",
      "iteration 400 / 1500: loss 2.251313\n",
      "iteration 500 / 1500: loss 2.235995\n",
      "iteration 600 / 1500: loss 2.220660\n",
      "iteration 700 / 1500: loss 2.211416\n",
      "iteration 800 / 1500: loss 2.268796\n",
      "iteration 900 / 1500: loss 2.266470\n",
      "iteration 1000 / 1500: loss 2.191992\n",
      "iteration 1100 / 1500: loss 2.261462\n",
      "iteration 1200 / 1500: loss 2.243559\n",
      "iteration 1300 / 1500: loss 2.250009\n",
      "iteration 1400 / 1500: loss 2.229062\n",
      "iteration 0 / 1500: loss 6152.199901\n",
      "iteration 100 / 1500: loss 2.293981\n",
      "iteration 200 / 1500: loss 2.314985\n",
      "iteration 300 / 1500: loss 2.288392\n",
      "iteration 400 / 1500: loss 2.311074\n",
      "iteration 500 / 1500: loss 2.305626\n",
      "iteration 600 / 1500: loss 2.257809\n",
      "iteration 700 / 1500: loss 2.305445\n",
      "iteration 800 / 1500: loss 2.268906\n",
      "iteration 900 / 1500: loss 2.295036\n",
      "iteration 1000 / 1500: loss 2.293995\n",
      "iteration 1100 / 1500: loss 2.333110\n",
      "iteration 1200 / 1500: loss 2.319781\n",
      "iteration 1300 / 1500: loss 2.278580\n",
      "iteration 1400 / 1500: loss 2.307063\n",
      "iteration 0 / 1500: loss 159.294637\n",
      "iteration 100 / 1500: loss 2.028433\n",
      "iteration 200 / 1500: loss 2.154493\n",
      "iteration 300 / 1500: loss 1.970491\n",
      "iteration 400 / 1500: loss 1.988802\n",
      "iteration 500 / 1500: loss 1.977522\n",
      "iteration 600 / 1500: loss 2.114503\n",
      "iteration 700 / 1500: loss 2.075556\n",
      "iteration 800 / 1500: loss 2.178230\n",
      "iteration 900 / 1500: loss 1.897463\n",
      "iteration 1000 / 1500: loss 1.876278\n",
      "iteration 1100 / 1500: loss 2.001101\n",
      "iteration 1200 / 1500: loss 2.076979\n",
      "iteration 1300 / 1500: loss 2.017372\n",
      "iteration 1400 / 1500: loss 2.041824\n",
      "iteration 0 / 1500: loss 67.820790\n",
      "iteration 100 / 1500: loss 4.455789\n",
      "iteration 200 / 1500: loss 2.017492\n",
      "iteration 300 / 1500: loss 1.889149\n",
      "iteration 400 / 1500: loss 1.903068\n",
      "iteration 500 / 1500: loss 1.916328\n",
      "iteration 600 / 1500: loss 1.978111\n",
      "iteration 700 / 1500: loss 1.892794\n",
      "iteration 800 / 1500: loss 1.921781\n",
      "iteration 900 / 1500: loss 1.945037\n",
      "iteration 1000 / 1500: loss 1.898215\n",
      "iteration 1100 / 1500: loss 1.852063\n",
      "iteration 1200 / 1500: loss 1.880705\n",
      "iteration 1300 / 1500: loss 1.981267\n",
      "iteration 1400 / 1500: loss 1.947975\n",
      "iteration 0 / 1500: loss 35.977684\n",
      "iteration 100 / 1500: loss 7.997501\n",
      "iteration 200 / 1500: loss 3.133200\n",
      "iteration 300 / 1500: loss 2.155518\n",
      "iteration 400 / 1500: loss 2.007277\n",
      "iteration 500 / 1500: loss 1.869175\n",
      "iteration 600 / 1500: loss 1.869085\n",
      "iteration 700 / 1500: loss 1.874427\n",
      "iteration 800 / 1500: loss 1.886785\n",
      "iteration 900 / 1500: loss 1.869058\n",
      "iteration 1000 / 1500: loss 1.820132\n",
      "iteration 1100 / 1500: loss 1.875717\n",
      "iteration 1200 / 1500: loss 2.049103\n",
      "iteration 1300 / 1500: loss 1.988489\n",
      "iteration 1400 / 1500: loss 1.775047\n",
      "iteration 0 / 1500: loss 309.524611\n",
      "iteration 100 / 1500: loss 2.132538\n",
      "iteration 200 / 1500: loss 2.040665\n",
      "iteration 300 / 1500: loss 2.186215\n",
      "iteration 400 / 1500: loss 2.177208\n",
      "iteration 500 / 1500: loss 2.043379\n",
      "iteration 600 / 1500: loss 2.067111\n",
      "iteration 700 / 1500: loss 2.254097\n",
      "iteration 800 / 1500: loss 2.136222\n",
      "iteration 900 / 1500: loss 2.126520\n",
      "iteration 1000 / 1500: loss 2.055347\n",
      "iteration 1100 / 1500: loss 2.080636\n",
      "iteration 1200 / 1500: loss 2.132241\n",
      "iteration 1300 / 1500: loss 2.134190\n",
      "iteration 1400 / 1500: loss 2.186136\n",
      "iteration 0 / 1500: loss 636.892944\n",
      "iteration 100 / 1500: loss 2.349052\n",
      "iteration 200 / 1500: loss 2.215359\n",
      "iteration 300 / 1500: loss 2.206731\n",
      "iteration 400 / 1500: loss 2.110863\n",
      "iteration 500 / 1500: loss 2.268131\n",
      "iteration 600 / 1500: loss 2.136662\n",
      "iteration 700 / 1500: loss 2.180572\n",
      "iteration 800 / 1500: loss 2.209913\n",
      "iteration 900 / 1500: loss 2.169614\n",
      "iteration 1000 / 1500: loss 2.112859\n",
      "iteration 1100 / 1500: loss 2.138029\n",
      "iteration 1200 / 1500: loss 2.281205\n",
      "iteration 1300 / 1500: loss 2.200087\n",
      "iteration 1400 / 1500: loss 2.114059\n",
      "iteration 0 / 1500: loss 1528.510373\n",
      "iteration 100 / 1500: loss 2.293581\n",
      "iteration 200 / 1500: loss 2.271804\n",
      "iteration 300 / 1500: loss 2.297965\n",
      "iteration 400 / 1500: loss 2.232674\n",
      "iteration 500 / 1500: loss 2.457605\n",
      "iteration 600 / 1500: loss 2.480337\n",
      "iteration 700 / 1500: loss 2.275332\n",
      "iteration 800 / 1500: loss 2.234140\n",
      "iteration 900 / 1500: loss 2.427485\n",
      "iteration 1000 / 1500: loss 2.426643\n",
      "iteration 1100 / 1500: loss 2.293304\n",
      "iteration 1200 / 1500: loss 2.330887\n",
      "iteration 1300 / 1500: loss 2.397180\n",
      "iteration 1400 / 1500: loss 2.589065\n",
      "iteration 0 / 1500: loss 3058.889445\n",
      "iteration 100 / 1500: loss 3.637847\n",
      "iteration 200 / 1500: loss 4.215807\n",
      "iteration 300 / 1500: loss 4.106869\n",
      "iteration 400 / 1500: loss 4.453938\n",
      "iteration 500 / 1500: loss 3.269653\n",
      "iteration 600 / 1500: loss 3.997706\n",
      "iteration 700 / 1500: loss 3.872587\n",
      "iteration 800 / 1500: loss 3.217137\n",
      "iteration 900 / 1500: loss 3.999663\n",
      "iteration 1000 / 1500: loss 4.396016\n",
      "iteration 1100 / 1500: loss 4.995104\n",
      "iteration 1200 / 1500: loss 4.158519\n",
      "iteration 1300 / 1500: loss 3.969660\n",
      "iteration 1400 / 1500: loss 4.190892\n",
      "iteration 0 / 1500: loss 6203.133643\n",
      "iteration 100 / 1500: loss 34.370891\n",
      "iteration 200 / 1500: loss 33.986366\n",
      "iteration 300 / 1500: loss 33.536333\n",
      "iteration 400 / 1500: loss 33.224596\n",
      "iteration 500 / 1500: loss 34.415037\n",
      "iteration 600 / 1500: loss 34.540104\n",
      "iteration 700 / 1500: loss 34.696940\n",
      "iteration 800 / 1500: loss 33.378840\n",
      "iteration 900 / 1500: loss 32.626241\n",
      "iteration 1000 / 1500: loss 32.675497\n",
      "iteration 1100 / 1500: loss 31.709052\n",
      "iteration 1200 / 1500: loss 34.790578\n",
      "iteration 1300 / 1500: loss 33.654057\n",
      "iteration 1400 / 1500: loss 30.409787\n",
      "iteration 0 / 1500: loss 158.136652\n",
      "iteration 100 / 1500: loss 3.406431\n",
      "iteration 200 / 1500: loss 3.274831\n",
      "iteration 300 / 1500: loss 3.937132\n",
      "iteration 400 / 1500: loss 3.114813\n",
      "iteration 500 / 1500: loss 2.991474\n",
      "iteration 600 / 1500: loss 2.593265\n",
      "iteration 700 / 1500: loss 4.374050\n",
      "iteration 800 / 1500: loss 3.194711\n",
      "iteration 900 / 1500: loss 3.553320\n",
      "iteration 1000 / 1500: loss 3.817102\n",
      "iteration 1100 / 1500: loss 3.052895\n",
      "iteration 1200 / 1500: loss 2.574313\n",
      "iteration 1300 / 1500: loss 3.310882\n",
      "iteration 1400 / 1500: loss 2.436566\n",
      "iteration 0 / 1500: loss 65.590079\n",
      "iteration 100 / 1500: loss 2.687813\n",
      "iteration 200 / 1500: loss 3.035846\n",
      "iteration 300 / 1500: loss 2.486909\n",
      "iteration 400 / 1500: loss 3.622668\n",
      "iteration 500 / 1500: loss 2.289743\n",
      "iteration 600 / 1500: loss 2.506108\n",
      "iteration 700 / 1500: loss 2.068777\n",
      "iteration 800 / 1500: loss 3.922075\n",
      "iteration 900 / 1500: loss 3.227964\n",
      "iteration 1000 / 1500: loss 2.355063\n",
      "iteration 1100 / 1500: loss 2.693009\n",
      "iteration 1200 / 1500: loss 2.739291\n",
      "iteration 1300 / 1500: loss 2.435510\n",
      "iteration 1400 / 1500: loss 2.669279\n",
      "iteration 0 / 1500: loss 35.571740\n",
      "iteration 100 / 1500: loss 4.546165\n",
      "iteration 200 / 1500: loss 3.386736\n",
      "iteration 300 / 1500: loss 2.789957\n",
      "iteration 400 / 1500: loss 2.583839\n",
      "iteration 500 / 1500: loss 2.002440\n",
      "iteration 600 / 1500: loss 2.799776\n",
      "iteration 700 / 1500: loss 3.222827\n",
      "iteration 800 / 1500: loss 3.576877\n",
      "iteration 900 / 1500: loss 2.373937\n",
      "iteration 1000 / 1500: loss 3.021656\n",
      "iteration 1100 / 1500: loss 3.027509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.289806\n",
      "iteration 1300 / 1500: loss 2.791898\n",
      "iteration 1400 / 1500: loss 2.818310\n",
      "iteration 0 / 1500: loss 313.335638\n",
      "iteration 100 / 1500: loss 3.456576\n",
      "iteration 200 / 1500: loss 3.618520\n",
      "iteration 300 / 1500: loss 4.052682\n",
      "iteration 400 / 1500: loss 4.334271\n",
      "iteration 500 / 1500: loss 4.896584\n",
      "iteration 600 / 1500: loss 3.189664\n",
      "iteration 700 / 1500: loss 3.286817\n",
      "iteration 800 / 1500: loss 4.209933\n",
      "iteration 900 / 1500: loss 4.784416\n",
      "iteration 1000 / 1500: loss 2.896632\n",
      "iteration 1100 / 1500: loss 2.765634\n",
      "iteration 1200 / 1500: loss 3.360772\n",
      "iteration 1300 / 1500: loss 3.846934\n",
      "iteration 1400 / 1500: loss 4.482263\n",
      "iteration 0 / 1500: loss 617.979744\n",
      "iteration 100 / 1500: loss 5.402758\n",
      "iteration 200 / 1500: loss 5.187013\n",
      "iteration 300 / 1500: loss 4.348413\n",
      "iteration 400 / 1500: loss 6.843622\n",
      "iteration 500 / 1500: loss 4.690848\n",
      "iteration 600 / 1500: loss 5.013499\n",
      "iteration 700 / 1500: loss 5.210719\n",
      "iteration 800 / 1500: loss 4.134927\n",
      "iteration 900 / 1500: loss 4.820897\n",
      "iteration 1000 / 1500: loss 4.209648\n",
      "iteration 1100 / 1500: loss 6.016265\n",
      "iteration 1200 / 1500: loss 4.156335\n",
      "iteration 1300 / 1500: loss 4.596587\n",
      "iteration 1400 / 1500: loss 4.873502\n",
      "iteration 0 / 1500: loss 1526.664672\n",
      "iteration 100 / 1500: loss 7.440801\n",
      "iteration 200 / 1500: loss 10.806438\n",
      "iteration 300 / 1500: loss 8.331139\n",
      "iteration 400 / 1500: loss 9.609995\n",
      "iteration 500 / 1500: loss 8.474162\n",
      "iteration 600 / 1500: loss 11.311762\n",
      "iteration 700 / 1500: loss 9.864940\n",
      "iteration 800 / 1500: loss 10.030103\n",
      "iteration 900 / 1500: loss 8.359154\n",
      "iteration 1000 / 1500: loss 8.134614\n",
      "iteration 1100 / 1500: loss 9.348723\n",
      "iteration 1200 / 1500: loss 8.828440\n",
      "iteration 1300 / 1500: loss 9.823575\n",
      "iteration 1400 / 1500: loss 10.945555\n",
      "iteration 0 / 1500: loss 3040.568812\n",
      "iteration 100 / 1500: loss 69.900014\n",
      "iteration 200 / 1500: loss 69.158391\n",
      "iteration 300 / 1500: loss 65.055209\n",
      "iteration 400 / 1500: loss 72.910003\n",
      "iteration 500 / 1500: loss 67.220941\n",
      "iteration 600 / 1500: loss 64.216303\n",
      "iteration 700 / 1500: loss 71.474500\n",
      "iteration 800 / 1500: loss 67.757673\n",
      "iteration 900 / 1500: loss 65.318830\n",
      "iteration 1000 / 1500: loss 72.248385\n",
      "iteration 1100 / 1500: loss 76.092720\n",
      "iteration 1200 / 1500: loss 75.646068\n",
      "iteration 1300 / 1500: loss 75.248190\n",
      "iteration 1400 / 1500: loss 75.953934\n",
      "iteration 0 / 1500: loss 6158.835233\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.163020 val accuracy: 0.178000\n",
      "lr 1.000000e-08 reg 2.000000e+03 train accuracy: 0.144388 val accuracy: 0.131000\n",
      "lr 1.000000e-08 reg 5.000000e+03 train accuracy: 0.146041 val accuracy: 0.178000\n",
      "lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.150837 val accuracy: 0.143000\n",
      "lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.173000 val accuracy: 0.171000\n",
      "lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.211612 val accuracy: 0.225000\n",
      "lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.263082 val accuracy: 0.271000\n",
      "lr 1.000000e-08 reg 2.000000e+05 train accuracy: 0.267490 val accuracy: 0.274000\n",
      "lr 5.000000e-08 reg 1.000000e+03 train accuracy: 0.238347 val accuracy: 0.231000\n",
      "lr 5.000000e-08 reg 2.000000e+03 train accuracy: 0.236143 val accuracy: 0.249000\n",
      "lr 5.000000e-08 reg 5.000000e+03 train accuracy: 0.254245 val accuracy: 0.242000\n",
      "lr 5.000000e-08 reg 1.000000e+04 train accuracy: 0.290306 val accuracy: 0.284000\n",
      "lr 5.000000e-08 reg 2.000000e+04 train accuracy: 0.327837 val accuracy: 0.347000\n",
      "lr 5.000000e-08 reg 5.000000e+04 train accuracy: 0.302531 val accuracy: 0.318000\n",
      "lr 5.000000e-08 reg 1.000000e+05 train accuracy: 0.286327 val accuracy: 0.298000\n",
      "lr 5.000000e-08 reg 2.000000e+05 train accuracy: 0.272918 val accuracy: 0.277000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.271000 val accuracy: 0.272000\n",
      "lr 1.000000e-07 reg 2.000000e+03 train accuracy: 0.285816 val accuracy: 0.298000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.331694 val accuracy: 0.333000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.355306 val accuracy: 0.380000\n",
      "lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.333592 val accuracy: 0.341000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.310102 val accuracy: 0.322000\n",
      "lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.291816 val accuracy: 0.301000\n",
      "lr 1.000000e-07 reg 2.000000e+05 train accuracy: 0.274673 val accuracy: 0.292000\n",
      "lr 2.000000e-07 reg 1.000000e+03 train accuracy: 0.309939 val accuracy: 0.308000\n",
      "lr 2.000000e-07 reg 2.000000e+03 train accuracy: 0.346306 val accuracy: 0.357000\n",
      "lr 2.000000e-07 reg 5.000000e+03 train accuracy: 0.370061 val accuracy: 0.396000\n",
      "lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.356837 val accuracy: 0.373000\n",
      "lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.333347 val accuracy: 0.355000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.304429 val accuracy: 0.318000\n",
      "lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.289347 val accuracy: 0.302000\n",
      "lr 2.000000e-07 reg 2.000000e+05 train accuracy: 0.262163 val accuracy: 0.274000\n",
      "lr 3.000000e-07 reg 1.000000e+03 train accuracy: 0.352980 val accuracy: 0.368000\n",
      "lr 3.000000e-07 reg 2.000000e+03 train accuracy: 0.379592 val accuracy: 0.397000\n",
      "lr 3.000000e-07 reg 5.000000e+03 train accuracy: 0.375796 val accuracy: 0.391000\n",
      "lr 3.000000e-07 reg 1.000000e+04 train accuracy: 0.354816 val accuracy: 0.369000\n",
      "lr 3.000000e-07 reg 2.000000e+04 train accuracy: 0.335694 val accuracy: 0.340000\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.297143 val accuracy: 0.317000\n",
      "lr 3.000000e-07 reg 1.000000e+05 train accuracy: 0.274184 val accuracy: 0.283000\n",
      "lr 3.000000e-07 reg 2.000000e+05 train accuracy: 0.260367 val accuracy: 0.282000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.385939 val accuracy: 0.392000\n",
      "lr 5.000000e-07 reg 2.000000e+03 train accuracy: 0.390224 val accuracy: 0.397000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.370286 val accuracy: 0.383000\n",
      "lr 5.000000e-07 reg 1.000000e+04 train accuracy: 0.354592 val accuracy: 0.365000\n",
      "lr 5.000000e-07 reg 2.000000e+04 train accuracy: 0.337878 val accuracy: 0.351000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.285633 val accuracy: 0.293000\n",
      "lr 5.000000e-07 reg 1.000000e+05 train accuracy: 0.286592 val accuracy: 0.303000\n",
      "lr 5.000000e-07 reg 2.000000e+05 train accuracy: 0.280408 val accuracy: 0.292000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.394694 val accuracy: 0.398000\n",
      "lr 1.000000e-06 reg 2.000000e+03 train accuracy: 0.386286 val accuracy: 0.389000\n",
      "lr 1.000000e-06 reg 5.000000e+03 train accuracy: 0.356306 val accuracy: 0.362000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.347673 val accuracy: 0.359000\n",
      "lr 1.000000e-06 reg 2.000000e+04 train accuracy: 0.321020 val accuracy: 0.336000\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.293531 val accuracy: 0.302000\n",
      "lr 1.000000e-06 reg 1.000000e+05 train accuracy: 0.258388 val accuracy: 0.280000\n",
      "lr 1.000000e-06 reg 2.000000e+05 train accuracy: 0.246204 val accuracy: 0.257000\n",
      "lr 2.000000e-06 reg 1.000000e+03 train accuracy: 0.393918 val accuracy: 0.401000\n",
      "lr 2.000000e-06 reg 2.000000e+03 train accuracy: 0.368980 val accuracy: 0.371000\n",
      "lr 2.000000e-06 reg 5.000000e+03 train accuracy: 0.354429 val accuracy: 0.366000\n",
      "lr 2.000000e-06 reg 1.000000e+04 train accuracy: 0.328959 val accuracy: 0.347000\n",
      "lr 2.000000e-06 reg 2.000000e+04 train accuracy: 0.312857 val accuracy: 0.327000\n",
      "lr 2.000000e-06 reg 5.000000e+04 train accuracy: 0.276163 val accuracy: 0.281000\n",
      "lr 2.000000e-06 reg 1.000000e+05 train accuracy: 0.226184 val accuracy: 0.221000\n",
      "lr 2.000000e-06 reg 2.000000e+05 train accuracy: 0.223694 val accuracy: 0.214000\n",
      "lr 4.000000e-06 reg 1.000000e+03 train accuracy: 0.373102 val accuracy: 0.382000\n",
      "lr 4.000000e-06 reg 2.000000e+03 train accuracy: 0.349449 val accuracy: 0.342000\n",
      "lr 4.000000e-06 reg 5.000000e+03 train accuracy: 0.309000 val accuracy: 0.305000\n",
      "lr 4.000000e-06 reg 1.000000e+04 train accuracy: 0.299204 val accuracy: 0.303000\n",
      "lr 4.000000e-06 reg 2.000000e+04 train accuracy: 0.262714 val accuracy: 0.249000\n",
      "lr 4.000000e-06 reg 5.000000e+04 train accuracy: 0.218510 val accuracy: 0.217000\n",
      "lr 4.000000e-06 reg 1.000000e+05 train accuracy: 0.120796 val accuracy: 0.127000\n",
      "lr 4.000000e-06 reg 2.000000e+05 train accuracy: 0.080857 val accuracy: 0.094000\n",
      "lr 8.000000e-06 reg 1.000000e+03 train accuracy: 0.297714 val accuracy: 0.300000\n",
      "lr 8.000000e-06 reg 2.000000e+03 train accuracy: 0.206449 val accuracy: 0.220000\n",
      "lr 8.000000e-06 reg 5.000000e+03 train accuracy: 0.249776 val accuracy: 0.264000\n",
      "lr 8.000000e-06 reg 1.000000e+04 train accuracy: 0.223469 val accuracy: 0.220000\n",
      "lr 8.000000e-06 reg 2.000000e+04 train accuracy: 0.159551 val accuracy: 0.171000\n",
      "lr 8.000000e-06 reg 5.000000e+04 train accuracy: 0.130245 val accuracy: 0.114000\n",
      "lr 8.000000e-06 reg 1.000000e+05 train accuracy: 0.076408 val accuracy: 0.063000\n",
      "lr 8.000000e-06 reg 2.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+03 train accuracy: 0.208673 val accuracy: 0.199000\n",
      "lr 5.000000e-05 reg 2.000000e+03 train accuracy: 0.143469 val accuracy: 0.139000\n",
      "lr 5.000000e-05 reg 5.000000e+03 train accuracy: 0.153163 val accuracy: 0.151000\n",
      "lr 5.000000e-05 reg 1.000000e+04 train accuracy: 0.103224 val accuracy: 0.099000\n",
      "lr 5.000000e-05 reg 2.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 2.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.401000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [0.5e-7,1e-7,2e-7,3e-7,5e-7,1e-8,5e-5,1e-6,2e-6,4e-6,8e-6]\n",
    "regularization_strengths = [5e3,2e3,1e3,1e4,2e4,5e4,1e5,2e5]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "for learning_rt in learning_rates:\n",
    "    for reg_str in regularization_strengths:\n",
    "            softmax = Softmax()\n",
    "            loss_hist = softmax.train(X_train, y_train, learning_rate=learning_rt, reg=reg_str,\n",
    "                      num_iters=1500, verbose=True)\n",
    "            y_train_pred = softmax.predict(X_train)\n",
    "            training_accuracy = np.mean(y_train == y_train_pred)\n",
    "            y_val_pred = softmax.predict(X_val)\n",
    "            validation_accuracy = np.mean(y_val == y_val_pred)\n",
    "            results[(learning_rt,reg_str)] = (training_accuracy,validation_accuracy)\n",
    "            if(validation_accuracy>best_val):\n",
    "                best_val = validation_accuracy\n",
    "                best_softmax = softmax\n",
    "\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb37cc6",
   "metadata": {
    "test": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.384000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df501314",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n",
    "True because softmax is probability based. Therefore it will never give 0 loss even for true image. This is not the case with SVM. SVM just wants correct image score to be higher than other scores + delta \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade33adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d17f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
